

from datetime import datetime, date
from sqlalchemy.orm import Session,aliased
from models import *  
from database import SessionLocal, engine, Base
import time
import random
from decimal import Decimal
from sqlalchemy import desc
import psycopg2

from scraping_simplicado  import run , delay
from sqlalchemy import select, func, nullsfirst
from sqlalchemy import asc

import os




def criar_novo_historico(db: Session, numero_de_links: int):
    try:
        # Verifica se j√° existe um hist√≥rico interrompido mais recente
        historico_interrompido = (
            db.query(ScrapingHistorico)
            .filter(ScrapingHistorico.status == "interrompido")
            .order_by(desc(ScrapingHistorico.data_scraping), desc(ScrapingHistorico.inicio_execucao))
            .first()
        )

        if historico_interrompido:
            return historico_interrompido , False

        # Se n√£o existir, cria um novo
        novo_historico = ScrapingHistorico(
            data_scraping=date.today(),
            inicio_execucao=datetime.utcnow(),
            numero_de_links=numero_de_links,
            status="em_andamento"
        )

        db.add(novo_historico)
        db.commit()
        db.refresh(novo_historico)
        return novo_historico , True

    except Exception as e:
        print(f"Erro ao criar hist√≥rico: {e}")
        db.rollback()
        raise

def salvar_scraping(db: Session, id_historico: int, id_produto: int, resposta: dict):
   
    
    
    try:    
        scraping = Scraping(
                id_scraping_historico=id_historico,
                id_produto=id_produto,
                link_ativo=resposta.get("link_ativo", None),
                nome_produto=resposta.get("nome_produto", None),
                preco_produto=resposta.get("preco_produto",0),
                vendedor=resposta.get("vendedor", None),
                tag_sem_estoque=resposta.get("tag_sem_estoque", False),
                tag_ultimas_unidades=resposta.get("tag_ultimas_unidades", False),
                descricao_erro=resposta.get("descricao_erro", None),
        )

        

        db.add(scraping)
        db.commit()
        db.refresh(scraping)

        print(f"[DEBUG] Scraping salvo. ID: {scraping.id}")
        return scraping
    except Exception as e:
        print(f"Erro no produto {id_produto}: {e}")
        db.rollback()


def processar_todos_produtos(db: Session):
    TAMANHO_BLOCO = 30

    # --- sub‚Äëquery: para pegar a √∫ltima execu√ß√£o de cada produto ------------
    subquery_ultimos_produtos = (
        select(
            Scraping.id_produto,
            func.max(Scraping.data_criacao).label("ultima_execucao")
        )
        .group_by(Scraping.id_produto)
        .subquery()
    )

    s = aliased(subquery_ultimos_produtos)

    stmt = (
        select(Produto)
        .outerjoin(s, Produto.id == s.c.id_produto)
        .order_by(nullsfirst(asc(s.c.ultima_execucao)))
        .limit(500)
    )

    produtos = db.scalars(stmt).all()

        
    total_produtos = len(produtos)

    if total_produtos == 0:
        print("Nenhum produto encontrado.")
        return

    # 2. Cria hist√≥rico de execu√ß√£o
    historico, novo_historico = criar_novo_historico(db, total_produtos)
    
    if(not novo_historico):
        subquery = (
            db.query(Scraping.id_produto)
            .filter(Scraping.id_scraping_historico == historico.id)
            .subquery()
        )

        produtos = (
            db.query(Produto)
            .filter(~Produto.id.in_(subquery))
            .order_by(Produto.id)
            .all()
        )
        
    casos_erros = 0
    produtos_processados = 0
    total_produtos=len(produtos)

    try:
        # 3. Divide em blocos e processa
        for i in range(0, 500, TAMANHO_BLOCO):
            bloco = produtos[i:i + TAMANHO_BLOCO]

            for produto in bloco:
                produtos_processados += 1
                print(f"Progresso: {produtos_processados}/{total_produtos} "
                      f"({produtos_processados/total_produtos:.1%}) - ID: {produto.id}")

                try:
                    resposta = run(produto.sku_marketplace, max_tentativas=3)

                    if resposta and resposta.get('status'):
                        salvar_scraping(
                            db=db,
                            id_historico=historico.id,
                            id_produto=produto.id,
                            resposta=resposta['response']

                        )
                    else:
                        casos_erros += 1
                        print(f"‚ùå Erro ao processar produto ID: {produto.id}")
                        time.sleep(random.uniform(250, 400))

                    
                except Exception as e:
                    casos_erros += 1
                    print(f"‚ùå Erro inesperado no produto ID {produto.id}: {str(e)}")

                time.sleep(random.uniform(30, 40))

            # Delay entre blocos
            print(f"\n‚è±Ô∏è Aguardando {10:.1f} segundos antes do pr√≥ximo bloco...")
            time.sleep(random.uniform(200,300 ))
            

    except Exception as e:
        print(f"‚ùå Erro durante o processamento: {str(e)}")
        historico.status = "interrompido"

    finally:
        historico.numero_de_links = db.query(Scraping).filter(Scraping.id_scraping_historico == historico.id).count()
        historico.fim_execucao = datetime.utcnow()
        historico.numero_erros = casos_erros
        historico.status = "finalizado" if produtos_processados == total_produtos else "interrompido"
        db.commit()
        print(f"\n‚úÖ Processamento conclu√≠do com {casos_erros} erro(s).")
        print(f"üîö √öltimo ID processado: {produtos_processados}")
        print(f"üì¶ √öltimo ID da lista: {produtos[-1].id if produtos else 'N/A'}")


def processar_todos_produtos(db: Session):
    TAMANHO_BLOCO = 30
    LIMITE_DA_BUSCA = 250 

    # 1. Subconsulta para encontrar a data da √∫ltima pesquisa de cada produto.
    #    Esta l√≥gica est√° correta e ser√° usada em ambos os cen√°rios.
    subquery_ultimos_produtos = (
        select(
            Scraping.id_produto,
            func.max(Scraping.data_criacao).label("ultima_execucao")
        )
        .group_by(Scraping.id_produto)
        .subquery()
    )
    s = aliased(subquery_ultimos_produtos)

    # 2. Verifica se existe uma execu√ß√£o para ser retomada.
    historico, novo_historico = criar_novo_historico(db, LIMITE_DA_BUSCA)
    
    # 3. Constr√≥i a query base com a l√≥gica de ordena√ß√£o correta.
    query_base = (
        select(Produto)
        .outerjoin(s, Produto.id == s.c.id_produto)
        .order_by(nullsfirst(asc(s.c.ultima_execucao)))
    )

    # 4. Se estivermos retomando um trabalho, adicionamos um filtro extra.
    if not novo_historico:
        print(f"Retomando execu√ß√£o interrompida do hist√≥rico ID: {historico.id}")
        
        # Subquery para pegar IDs de produtos J√Å PROCESSADOS neste hist√≥rico
        produtos_ja_processados_subquery = (
            select(Scraping.id_produto)
            .filter(Scraping.id_scraping_historico == historico.id)
            .scalar_subquery() # Usar scalar_subquery para usar com .not_in()
        )
        
        # Adiciona o filtro √† query base para excluir os produtos j√° processados
        query_final = query_base.filter(
            Produto.id.not_in(produtos_ja_processados_subquery)
        )
    else:
        print("Iniciando nova execu√ß√£o de scraping.")
        # Se for uma nova execu√ß√£o, a query base j√° est√° correta.
        query_final = query_base

    # 5. Aplica o limite no final e executa a query.
    #    Agora o .limit() ser√° aplicado em ambos os casos (nova execu√ß√£o ou retomada).
    produtos = db.scalars(query_final.limit(LIMITE_DA_BUSCA)).all()
        
    total_produtos = len(produtos)

    if total_produtos == 0:
        print("Nenhum produto novo ou antigo para processar.")
        # Se for um hist√≥rico retomado, talvez todos j√° tenham sido processados.
        if not novo_historico:
            historico.status = "finalizado"
            historico.fim_execucao = datetime.utcnow()
            db.commit()
        return

    # Atualiza o n√∫mero de links no hist√≥rico caso seja uma nova execu√ß√£o.
    # Se for uma retomada, o n√∫mero total j√° foi definido quando foi criado.
    if novo_historico:
        historico.numero_de_links = total_produtos
        db.commit()
    
    casos_erros = 0
    # O n√∫mero de processados deve come√ßar a contar a partir do que j√° existe no hist√≥rico
    produtos_processados_nesta_execucao = 0
    total_a_processar_nesta_execucao = len(produtos)


    try:
        # 6. Processa os produtos em blocos
        for i in range(0, total_a_processar_nesta_execucao, TAMANHO_BLOCO):
            bloco = produtos[i:i + TAMANHO_BLOCO]

            for produto in bloco:
                produtos_processados_nesta_execucao += 1
                print(f"Progresso nesta sess√£o: {produtos_processados_nesta_execucao}/{total_a_processar_nesta_execucao} "
                      f"({produtos_processados_nesta_execucao/total_a_processar_nesta_execucao:.1%}) - ID do Produto: {produto.id}")

                try:
                    resposta = run(produto.sku_marketplace, max_tentativas=3)

                    if resposta and resposta.get('status'):
                        salvar_scraping(
                            db=db,
                            id_historico=historico.id,
                            id_produto=produto.id,
                            resposta=resposta['response']
                        )
                    else:
                        casos_erros += 1
                        print(f"‚ùå Erro ao processar produto ID: {produto.id}")
                        time.sleep(random.uniform(200, 250)) 

                except Exception as e:
                    casos_erros += 1
                    print(f"‚ùå Erro inesperado no produto ID {produto.id}: {str(e)}")

                time.sleep(random.uniform(30, 50)) 

            # Delay entre blocos
            if i + TAMANHO_BLOCO < total_a_processar_nesta_execucao:
                delay_bloco = random.uniform(20, 30)
                print(f"\n‚è±Ô∏è Fim do bloco. Aguardando {delay_bloco:.1f} segundos...")
                time.sleep(delay_bloco)
            
    except Exception as e:
        print(f"‚ùå Erro cr√≠tico durante o processamento: {str(e)}")
        historico.status = "interrompido"
    finally:
        # L√≥gica de finaliza√ß√£o do hist√≥rico
        total_processado_no_historico = db.query(Scraping).filter(Scraping.id_scraping_historico == historico.id).count()
        historico.numero_de_links = total_processado_no_historico 
        historico.fim_execucao = datetime.utcnow()
        historico.numero_erros = (historico.numero_erros or 0) + casos_erros
        
        if total_a_processar_nesta_execucao < LIMITE_DA_BUSCA:
             historico.status = "finalizado"
        else:
             historico.status = "interrompido" #

        db.commit()
        print("\n--- Resumo da Execu√ß√£o ---")
        print(f"Status do Hist√≥rico ({historico.id}): {historico.status}")
        print(f"Total de produtos processados nesta sess√£o: {produtos_processados_nesta_execucao}")
        print(f"Total de erros nesta sess√£o: {casos_erros}")
        print("--------------------------\n")

if __name__ == "__main__":
    db = SessionLocal()
    try:
        processar_todos_produtos(db)
    finally:
        db.close()